\documentclass[10pt]{article}
\setlength{\textwidth}{6.3in}
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\topmargin}{-.5in}
%\parindent=0in
\linespread{1.3}
\usepackage{ mathrsfs }
\usepackage{amsthm}
\usepackage{ amssymb }
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{fancyhdr}
\pagestyle{fancy}
\headheight = 14.5pt
\lhead{Statement of Purpose - Thomas Zeng}
\rhead{University of Chicago}
\cfoot{\thepage}

\usepackage{tikz}
\usetikzlibrary{positioning}
\begin{document}

%for why chicago, primarily mention chenhao tan
% also say unique labs around it (adjacent to these ideas)

My academic advisor once told me that while deep learning (DL) can be powerful, she still prefers to use more conventional methods for her research due to its lack of explainability. Her comment made me realize that although DL is a potent tool, it still has significant trade-offs that hinders its usage.
Specifically, DL models have a high capacity for generalizing patterns in data, but this comes at a trade off both in the lack of explainability of model predictions and reliance on the assumption that the data seen at inference time is i.i.d. with respect to the training data. This is especially problematic in domains such as medical imaging where explainability and fairness of model predictions is paramount; and furthermore some degree of domain-shift in data at inference time is expected. Hence, my current objective is to get a PhD to do research in industry with a focus on addressing this problem, i.e. creating robust, fair and explainable deep learning models.


I started self-studying machine learning at the end of my sophomore year of college when my interest in the intersection of linguistics and computer science led me to wonder how machine translation algorithms worked.
To gain practical experience, I pursued ML at a startup, SayKid, that used pre-built speech recognition models. There, I created a voice game for children premised on riddle solving using the Alexa Skills Kit. This experience underscored to me the necessity of designing robustly, as given the target audience of children, we needed to account for the unpredictability of how a child will interact with our game. At the same time, as the APIs I used abstracted away the ML model as a black box that takes in speech and returns text, it also gave me the desire to try research where I could work at a lower level with a greater focus on implementing and training models.

Hence, I started working with Professor David Liben-Nowell, who is studying human behavior through choice modeling. Here we quantified the effect of geographic location on people's choices -- specifically their ranking of US states by contribution to history. We used a Plackett-Luce model \cite{guiver2009bayesian} as a baseline and iterated upon it to test how a person's home state's geographic location affects their individual ranking.
While choice modeling is not ML, they both fundamentally involve data and model building.
%ties this part more explicitly in with my research interest
Furthermore, although choice models are limited in what they can describe, they are highly explainable due to the choice theories that underpin them. This is in contrast to data-driven DL models that are powerful but black-box due to their empirical nature.
I resonated with the intuitive nature of choice modeling and this juxtaposition propelled me to explore the problem of robustness and explainability in the context of DL. 


% re-edit this as it is too long and enumeration-y and U Chicago to the non U Chicago sop
To work more with robustness -- specifically in the medical imaging domain -- I participated in an REU program hosted by DePaul University. I was advised by Professor Daniela Raicu and focused on improving the domain-shift robustness of DL lung nodule classification models by identifying and training models over hidden stratification in types of lung nodules.
% i.e. with group distributionally robust optimization \cite{Sagawa*2020Distributionally}.
%wip
Specifically, I used both domain-driven and clustering methods to find semantically meaningful subgroupings of lung nodules. Then, I compared results from our baseline ResNet-18 models -- i.e. trained using transfer learning under the principle of Empirical Risk Minimization (ERM) -- to ResNet-18 models trained with group distributionally robust optimization (gDRO) \cite{Sagawa*2020Distributionally}. Through this, we concluded that we could increase model robustness in regard to the subgroups we discovered by using gDRO.
% my research abilities?
% The main difficulty in this experience was finding meaningful subgroupings of the data -- this specifically was as we were not radiologists. Thus solving these problems was very rewarding and one of the large driver for why I enjoy the research process.
%wip
In this experience, I was able to do research full time and was first author on a paper that was accepted \cite{zengNo2023}. This program fully convinced me that I wanted to do research
% and thus cemented my desire to pursue graduate school.
as I found the process of solving open-ended questions -- e.g. what exactly is a ``semantically meaningful'' stratification of our dataset -- a challenging and rewarding process.
It also again reinforced my interest in explainability and robustness as I saw first hand the need for more research in the subfields and how important it is in some ``high stakes'' domains e.g. the medical field where model predictions can greatly affect patient outcomes.
 
% While my journey into deep learning is still nascent, it has been fruitful and given me a clear direction.
To continue in this direction, I am now exploring this problem in other DL modalities -- specifically fairness in NLP. For my senior capstone project which is advised by Professor Anna Rafferty, I am looking at counterfactual fairness in language toxicity classification, re-implementing Counterfactual Logit Pairing \cite{garg2019counterfactual} and evaluating the robustness of these methods. Although language toxicity classification is an entirely different domain from my previous work of lung nodule malignancy classification, the same problem of robustness and fairness manifests here. Thus, in the future I would like to find more generalized methods to make deep learning models more transparent and accountable, 
%like how gdro and irm have been proposed?
as the opacity in deep learning models is a pervasive problem across different domains and modalities.
% I have also held ML workshops in the data science club at my school to help expose other people to ML in the hopes of bring other people to this field too.
%talk about 
% ROSA advice: mention more about what my future direction is?
To this end, I am applying for PhD programs as research is a fundamentally collaborative effort and not something I can develop on my own.
% Specifically, I wish to further develop my research ability in both finding meaningful questions and also producing useful results. 
This is with the goal of giving me the skills to eventually do research in industry e.g. with Google Jigsaw or OpenAI, where I can help ensure AI models being deployed in the world are used responsibly and equitably.
%centered on benefits and effects of ai on humans -- human centric research on ai

Therefore, I am applying to University of Chicago with an interest in working with Professor Chenhao Tan in the CHAI lab. His focus on  a ``machine-in-the-loop'' paradigm appeals to me as a fruitful lens in which to develop machine learning algorithms for tasks that are not entirely delegable to machines due to their high stakes nature. Furthermore, his lab's work on human-AI collaboration such as distribution shifts in human-AI decision-making \cite{liu2021understanding} and content delegation in content moderation \cite{lai2022human} aligns with my own research interests of explainability and robustness. By working in his lab, I can contribute with my knowledge of deep learning frameworks and experience working with domain robustness problems. Through his lab, I also aim to further my own goals of eventually working on robust machine learning models.


% And I believe his lab will help me along on my own goal.


% But besides the CHAI lab, my previous research was in Chicago and I greatly enjoyed the city. So I believe location wise, it is a good fit for me and would provide me with the right environment to thrive and develop as a researcher.%probably should elaborate more?

% I specifically am applying to University of Minnesota because of its sizeable faculty in machine learning and bioinformatics. I specifically would like to work with Professor Catherine Qi Zhao in the VIP Lab or with Professor Ju Sun in the GLOVEX lab. The former lab is of interest to me due to the work on interpratability and domain shift of models i.e. reasoning capability for attention \cite{chen2021attention}, domain adaptation for training an action recognition classifier \cite{li2017attention} or trustworthiness with steep slope loss \cite{Luo_NeurIPS_2021}. The latter lab is of interest to me due to the focus on healthcare and research in methods to improve classification performance e.g. medical transfer learning \cite{peng2021rethink} and evaluation of COVID-19 detection models \cite{doi:10.1148/ryai.210217}. 
% %need to reword this better
% Besides the faculty, I also enjoy the location of Minneapolis and the cold weather -- and ultimately I think that it can provide the right environment for me to thrive and develop as a researcher.




% \newpage
\bibliographystyle{acm}
\bibliography{refs} % Entries are in the refs.bib file



\end{document}
