\documentclass[10pt]{article}
\setlength{\textwidth}{6.3in}
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\topmargin}{-.5in}
%\parindent=0in
\linespread{1.3}
\usepackage{ mathrsfs }
\usepackage{amsthm}
\usepackage{ amssymb }
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{fancyhdr}
\pagestyle{fancy}
\headheight = 14.5pt
\lhead{Statement of Purpose - Thomas Zeng}
\rhead{Washington University in St. Louis}
\cfoot{\thepage}

\usepackage{tikz}
\usetikzlibrary{positioning}
\begin{document}

%for why chicago, primarily mention chenhao tan
% also say unique labs around it (adjacent to these ideas)

My academic advisor once told me that while deep learning (DL) can be powerful, she still prefers to use more conventional methods for her research due to its lack of explainability. Her comment made me realize that although DL is a potent tool, it still has significant trade-offs that hinders its usage.
Specifically, DL models have a high capacity for generalizing patterns in data, but this comes at a trade off both in the lack of explainability of model predictions and reliance on the assumption that the data seen at inference time is i.i.d. with respect to the training data. This is especially problematic in domains such as medical imaging where explainability and fairness of model predictions is paramount; and furthermore some degree of domain-shift in data at inference time is expected. Hence, my current objective is to get a PhD to do research in industry with a focus on addressing this problem, i.e. creating robust, fair and explainable deep learning models.


I started self-studying machine learning at the end of my sophomore year of college when my interest in the intersection of linguistics and computer science led me to wonder how machine translation algorithms worked.
To gain practical experience, I pursued ML at a startup, SayKid, that used pre-built speech recognition models. There, I created a voice game for children premised on riddle solving using the Alexa Skills Kit. This experience underscored to me the necessity of designing robustly, as given the target audience of children, we needed to account for the unpredictability of how a child will interact with our game. At the same time, as the APIs I used abstracted away the ML model as a black box that takes in speech and returns text, it also gave me the desire to try research where I could work at a lower level with a greater focus on implementing and training models.

Hence, I started working with Professor David Liben-Nowell, who is studying human behavior through choice modeling. Here we quantified the effect of geographic location on people's choices -- specifically their ranking of US states by contribution to history. We used a Plackett-Luce model \cite{guiver2009bayesian} as a baseline and iterated upon it to test how a person's home state's geographic location affects their individual ranking.
While choice modeling is not ML, they both fundamentally involve data and model building.
%ties this part more explicitly in with my research interest
Furthermore, although choice models are limited in what they can describe, they are highly explainable due to the choice theories that underpin them. This is in contrast to data-driven DL models that are powerful but black-box due to their empirical nature.
I resonated with the intuitive nature of choice modeling and this juxtaposition propelled me to explore the problem of robustness and explainability in the context of DL. 


% re-edit this as it is too long and enumeration-y and U Chicago to the non U Chicago sop
To work more with robustness -- specifically in the medical imaging domain -- I participated in an REU program hosted by DePaul University. I was advised by Professor Daniela Raicu and focused on improving the domain-shift robustness of DL lung nodule classification models by identifying and training models over hidden stratification in types of lung nodules.
% i.e. with group distributionally robust optimization \cite{Sagawa*2020Distributionally}.
%wip
Specifically, I used both domain-driven and clustering methods to find semantically meaningful subgroupings of lung nodules. Then, I compared results from our baseline ResNet-18 models -- i.e. trained using transfer learning under the principle of Empirical Risk Minimization (ERM) -- to ResNet-18 models trained with group distributionally robust optimization (gDRO) \cite{Sagawa*2020Distributionally}. Through this, we concluded that we could increase model robustness in regard to the subgroups we discovered by using gDRO.
% my research abilities?
% The main difficulty in this experience was finding meaningful subgroupings of the data -- this specifically was as we were not radiologists. Thus solving these problems was very rewarding and one of the large driver for why I enjoy the research process.
%wip
In this experience, I was able to do research full time and was first author on a paper that was accepted \cite{zengNo2023}. This program fully convinced me that I wanted to do research
% and thus cemented my desire to pursue graduate school.
as I found the process of solving open-ended questions -- e.g. what exactly is a ``semantically meaningful'' stratification of our dataset -- a challenging and rewarding process.
It also again reinforced my interest in explainability and robustness as I saw first hand the need for more research in the subfields and how important it is in some ``high stakes'' domains e.g. the medical field where model predictions can greatly affect patient outcomes.
 
% While my journey into deep learning is still nascent, it has been fruitful and given me a clear direction.
To continue in this direction, I am now exploring this problem in other DL modalities -- specifically fairness in NLP. For my senior capstone project which is advised by Professor Anna Rafferty, I am looking at counterfactual fairness in language toxicity classification, re-implementing Counterfactual Logit Pairing \cite{garg2019counterfactual} and evaluating the robustness of these methods. Although language toxicity classification is an entirely different domain from my previous work of lung nodule malignancy classification, the same problem of robustness and fairness manifests here. Thus, in the future I would like to find more generalized methods to make deep learning models more transparent and accountable, 
%like how gdro and irm have been proposed?
as the opacity in deep learning models is a pervasive problem across different domains and modalities.
% I have also held ML workshops in the data science club at my school to help expose other people to ML in the hopes of bring other people to this field too.
%talk about 
% ROSA advice: mention more about what my future direction is?
To this end, I am applying for PhD programs as research is a fundamentally collaborative effort and not something I can develop on my own.
% Specifically, I wish to further develop my research ability in both finding meaningful questions and also producing useful results. 
This is with the goal of giving me the skills to eventually do research in industry, where I can help ensure AI models being deployed in the world are used responsibly and equitably.
%centered on benefits and effects of ai on humans -- human centric research on ai

Therefore, I am applying to Washington University in St. Louis with an interest in working with either Professor Chengguang Wang or Professor Yixin Chen.
I am primarily interested in Professor Wang's work on ``responsible'' NLP models e.g. using lexical watermarks to protect intellectual property \cite{he2022protecting} or mitigating language model backdoors \cite{zhang2022fine}.
Additionally, I enjoy his work on creating more ``accessible'' models e.g. creating a light Transformer \cite{wang2020transformer} or more efficient finetuning \cite{shen2022palt}.
As for Professor Chen, I am broadly interested in his work in clinical and healthcare applications. More specifically, I like his work in interpretable drug synergy predictions \cite{dong2021interpretable} and fairness in predicting medical complications \cite{tripathi2020fairness}.
By working in either of the professors' labs, I can contribute with my knowledge of deep learning frameworks and strong coding experience.
I also aim to take advantage of the overlapping interests of their work and mine to further my own goals of working on machine learning models that are responsible in both fairness and interpretability.

% Chengguang Wang
    % responsible models:
        % Protecting Intellectual Property of Language Generation APIs with Lexical Watermark
        % Fine-mixing: Mitigating Backdoors in Fine-tuned Language Models
    % work on "efficient" models
        % Transformers on a diet
        % PALT: Parameter-Lite Transfer of Language Models for Knowledge Graph Completion
            % transfer learning
% Yixin Chen
    % explainability (and healthcare)
    % Interpretable Drug Synergy Prediction with Graph Neural Networks for Human-AI Collaboration in Healthcare
    % Explainable Machine Learning for Regime-Based Asset Allocation
    % (Un)fairness in Post-operative Complication Prediction Models





% \newpage
\bibliographystyle{acm}
\bibliography{refs} % Entries are in the refs.bib file



\end{document}
