\documentclass[10pt]{article}
\setlength{\textwidth}{6.3in}
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\topmargin}{-.5in}
%\parindent=0in
\linespread{1.3}
\usepackage{ mathrsfs }
\usepackage{amsthm}
\usepackage{ amssymb }
\usepackage{graphicx}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{fancyhdr}
\pagestyle{fancy}
\headheight = 14.5pt
\lhead{Probability HW5, Thomas Zeng }
\rhead{Math 240, Fall 2022}
\cfoot{\thepage}

\begin{document}
\section*{1}
%4.4
\subsection*{a}
As $X$ is uniform, this is just the arithmetic mean i.e. $\frac{-2+-1+0+1+2}{4}=0.$

\subsection*{b}
We use the law of the unconscious statistician
\begin{align*}
    E[e^X] &= e^{-2}P(-2) + e^{-1}(P-1) + e^0P(0) + e^1P(1) + e^2P(2)\\
    &=\frac{1}{5}(e^{-2}+e^{-1}+e^0+e^1+e^2)\\
    &\approx 2.32
\end{align*}

\subsection*{c}

\begin{align*}
    E[\frac{1}{X+3}] &= \frac{1}{-2+3}P(-2) + \frac{1}{-1+3}(P-1) + \frac{1}{0+3}P(0) + \frac{1}{1+3}P(1) + \frac{1}{2+3}P(2)\\
    &=\frac{1}{5}(1 + \frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\frac{1}{5})\\
    &\approx 0.46
\end{align*}

\section*{2}
%4.2

As the random variable is uniform each number in $\{a,...,b\}$ has equal chance of being choosen, and thus the expectation is equivalent to the arithmetic mean which here is $a + \frac{b-a}{2}$ since every number between $a$ and $b$ are in the set.

\section*{3}
%4.24

For a fair dice we know that $\mu=3.5.$ Let us define this random variable as $X.$
Using the definition, we have
\begin{align*}
    V[X] &= E[(X-\mu)^2]\\
    &= \sum_{x=1}^6\frac{1}{6}(x-3.5)^2\\
    &\approx2.92
\end{align*}
%E[X^2] - E[X]^2
Using the computational formula we have
\begin{align*}
    V[X] &= E[X^2]-E[X]^2\\
    &= \sum_{x=1}^6\frac{1}{6}x^2-3.5^2\\
    &\approx2.92
\end{align*}
I think both methods work fine.

\section*{4}
%4.26

\subsection*{a}
$2^2V[X]+3^2V[Y] = 4c+9d$

\subsection*{b}
$3^2V[X] + V[Y]=9c+d$

\section*{5}
%4.32

For each $k$ we have $E[X_k]=1\times\frac{1}{2} + (-1)\times\frac{1}{2}=0.$ Thus by linearity of expectation, we have that
\[E[S]=E[X_1]+...+E[X_k]=0\]
For each $k$ we have that $V[X_k]=\frac{1}{2}(1-0)^2 + \frac{1}{2}(-1-0)^2=1.$ Thus as each random variable is independent, we have
\[V[S]=V[X_1]+...+V[X_k]=k.\]

\section*{6}
\begin{align*}
    E[X] &= -nP(X=-n) + nP(X=n)+ (-n+1)P(X=(-n+1)) + (n-1)P(X=n-1)...\\
    &= -nP(X=n) + nP(X=n)+ (-n+1)P(X=n-1) + (n-1)P(X=n-1)...,\;\text{(by symmetry property)}\\
    &=0
\end{align*}

\section*{7}
\begin{align*}
    E[X]&=\sum_{k=1}^\infty k\frac{p^k}{-k\ln(1-p)}\\
    &=\sum_{k=1}^\infty-\frac{p^k}{\ln(1-p)}\\
    &=-\frac{1}{\ln(1-p)}\sum_{k=1}^\infty p^k\\
    &=-\frac{1}{\ln(1-p)}p\sum_{k=0}^\infty p^k\\
    &=-\frac{1}{\ln(1-p)}\frac{p}{(1-p)}
\end{align*}
 
\section*{8}
\subsection*{a}
The expected value is the same for as if it was a binomial distribution scaled by $\alpha$ (since the only difference is at $Z=0$ where the term would be $0$). Thus $E[Z]=\alpha np.$

\subsection*{b}
We have $E[I_O]=\alpha$ and $E[X]=np$ by definition of their distributions. Thus $E[Z]=E[I_OX]=E[I_O]E[X]=\alpha np.$

\subsection*{c}
$E[X^2] = V[X] + E[X]^2 = np(1-p) + (np)^2.$\\
$E[I_O^2] = V[I_O] + E[I_O]^2=\alpha(1-\alpha) + \alpha^2$\\
$V[I_OX] = E[I_O^2X^2] - E[I_OX]^2 = (\alpha(1-\alpha) + \alpha^2)(np(1-p) + (np)^2) - (\alpha np)^2$

\section*{9}
\begin{align*}
    E[e^{tX}] &= e^{t0}(1-p) + e^{t1}p\\
            &= (1-p) + e^{t}p
\end{align*}
\end{document}