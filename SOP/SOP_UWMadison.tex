\documentclass[10pt]{article}
\setlength{\textwidth}{6.3in}
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\topmargin}{-.5in}
%\parindent=0in
\linespread{1.3}
\usepackage{ mathrsfs }
\usepackage{amsthm}
\usepackage{ amssymb }
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{fancyhdr}
\pagestyle{fancy}
\headheight = 14.5pt
\lhead{Statement of Purpose - Thomas Zeng}
\rhead{University of Wisconsin - Madison}
\cfoot{\thepage}

\usepackage{tikz}
\usetikzlibrary{positioning}
\begin{document}

%for why chicago, primarily mention chenhao tan
% also say unique labs around it (adjacent to these ideas)

My academic advisor once told me that while deep learning (DL) can be powerful, she still prefers to use more conventional methods for her research due to its lack of explainability. Her comment made me realize that although DL is a potent tool, it still has significant trade-offs that hinders its usage.
Specifically, DL models have a high capacity for generalizing patterns in data, but this comes at a trade off both in the lack of explainability of model predictions and reliance on the assumption that the data seen at inference time is i.i.d. with respect to the training data. This is especially problematic in domains such as medical imaging where explainability and fairness of model predictions is paramount; and furthermore some degree of domain-shift in data at inference time is expected. Hence, my current objective is to get a PhD to do research in industry with a focus on addressing this problem, i.e. creating robust, fair and explainable deep learning models.


I started self-studying machine learning at the end of my sophomore year of college when my interest in the intersection of linguistics and computer science led me to wonder how machine translation algorithms worked.
To gain practical experience, I pursued ML at a startup, SayKid, that used pre-built speech recognition models. There, I created a voice game for children premised on riddle solving using the Alexa Skills Kit. This experience underscored to me the necessity of designing robustly, as given the target audience of children, we needed to account for the unpredictability of how a child will interact with our game. At the same time, as the APIs I used abstracted away the ML model as a black box that takes in speech and returns text, it also gave me the desire to try research where I could work at a lower level with a greater focus on implementing and training models.

Hence, I started working with Professor David Liben-Nowell, who is studying human behavior through choice modeling. Here we quantified the effect of geographic location on people's choices -- specifically their ranking of US states by contribution to history. We used a Plackett-Luce model \cite{guiver2009bayesian} as a baseline and iterated upon it to test how a person's home state's geographic location affects their individual ranking.
While choice modeling is not ML, they both fundamentally involve data and model building.
%ties this part more explicitly in with my research interest
Furthermore, although choice models are limited in what they can describe, they are highly explainable due to the choice theories that underpin them. This is in contrast to data-driven DL models that are powerful but black-box due to their empirical nature.
I resonated with the intuitive nature of choice modeling and this juxtaposition propelled me to explore the problem of robustness and explainability in the context of DL. 


% re-edit this as it is too long and enumeration-y and U Chicago to the non U Chicago sop
To work more with robustness -- specifically in the medical imaging domain -- I participated in an REU program hosted by DePaul University. I was advised by Professor Daniela Raicu and focused on improving the domain-shift robustness of DL lung nodule classification models by identifying and training models over hidden stratification in types of lung nodules.
% i.e. with group distributionally robust optimization \cite{Sagawa*2020Distributionally}.
%wip
Specifically, I used both domain-driven and clustering methods to find semantically meaningful subgroupings of lung nodules. Then, I compared results from our baseline ResNet-18 models -- i.e. trained using transfer learning under the principle of Empirical Risk Minimization (ERM) -- to ResNet-18 models trained with group distributionally robust optimization (gDRO) \cite{Sagawa*2020Distributionally}. Through this, we concluded that we could increase model robustness in regard to the subgroups we discovered by using gDRO.
% my research abilities?
% The main difficulty in this experience was finding meaningful subgroupings of the data -- this specifically was as we were not radiologists. Thus solving these problems was very rewarding and one of the large driver for why I enjoy the research process.
%wip
In this experience, I was able to do research full time and was first author on a paper that was accepted \cite{zengNo2023}. This program fully convinced me that I wanted to do research
% and thus cemented my desire to pursue graduate school.
as I found the process of solving open-ended questions -- e.g. what exactly is a ``semantically meaningful'' stratification of our dataset -- a challenging and rewarding process.
It also again reinforced my interest in explainability and robustness as I saw first hand the need for more research in the subfields and how important it is in some ``high stakes'' domains e.g. the medical field where model predictions can greatly affect patient outcomes.
 
% While my journey into deep learning is still nascent, it has been fruitful and given me a clear direction.
To continue in this direction, I am now exploring this problem in other DL modalities -- specifically fairness in NLP. For my senior capstone project which is advised by Professor Anna Rafferty, I am looking at counterfactual fairness in language toxicity classification, re-implementing Counterfactual Logit Pairing \cite{garg2019counterfactual} and evaluating the robustness of these methods. Although language toxicity classification is an entirely different domain from my previous work of lung nodule malignancy classification, the same problem of robustness and fairness manifests here. Thus, in the future I would like to find more generalized methods to make deep learning models more transparent and accountable, 
%like how gdro and irm have been proposed?
as the opacity in deep learning models is a pervasive problem across different domains and modalities.
% I have also held ML workshops in the data science club at my school to help expose other people to ML in the hopes of bring other people to this field too.
%talk about 
% ROSA advice: mention more about what my future direction is?
To this end, I am applying for PhD programs as research is a fundamentally collaborative effort and not something I can develop on my own.
% Specifically, I wish to further develop my research ability in both finding meaningful questions and also producing useful results. 
This is with the goal of giving me the skills to eventually do research in industry e.g. with Google Jigsaw or OpenAI, where I can help ensure AI models being deployed in the world are used responsibly and equitably.
%centered on benefits and effects of ai on humans -- human centric research on ai

Therefore, I am applying to University of Wisconsin - Madison as there are many professors here doing research related to my own focus.
Specifically, I am most interested in Professor Sharon Yixuan Li and Professor Junjie Hu. Professor Li's lab has a focus on learning with distribution shifts, which aligns well with both my past research work with medical imaging and also my future goals. Specifically, I enjoy her work on out of distribution detection \cite{sun2022dice, cai2023frequency}, neural network overconfidence \cite{wei2022mitigating} and partial label learning \cite{wang2022revise}.
% elaborate more on these, and how my previous work - similar but in medical domain/can see how the work could be benificial
By working in her lab, I can contribute with my knowledge of deep learning frameworks and experience working with domain robustness problems. %add?
Similarly, Professor Hu's lab also does work on domain shifts albeit with a main focus in NLP with multimodal and multilingual models. Of particular note to me is his work on phrase-based active learning \cite{hu2021phrase} and lexical induction \cite{hu-etal-2019-domain-adaptation}.
Thus, by working in his lab, I can similarly effectively further my research goals while also focusing more on NLP. % change this sentence lol

% Sharon Yixuan Li
    % Is Out-of-Distribution Detection Learnable? -- very mathematically rigourous?
    % Out-of-distribution Detection via Frequency-regularized Generative Models

% Junjie Hu 

% I specifically am applying to Unnp.787cnccvcfdasdniversity of Wisconsin Madison due to its strong focus on machine learning i.e. Machine Learning @ UW-Madison. Specifically I would like to work with professors such as Sharon Yixuan Li (DICE \cite{sun2022dice} or nearst neighbor OOD \cite{sun2022knn}) or Junjie Hu .

% \newpage
\bibliographystyle{acm}
\bibliography{refs} % Entries are in the refs.bib file



\end{document}
