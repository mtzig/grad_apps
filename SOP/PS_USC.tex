\documentclass[10pt]{article}
\setlength{\textwidth}{6.3in}
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\topmargin}{-.5in}
%\parindent=0in
\linespread{1.3}
\usepackage{ mathrsfs }
\usepackage{amsthm}
\usepackage{ amssymb }
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{fancyhdr}
\pagestyle{fancy}
\headheight = 14.5pt
\lhead{Personal Statement - Thomas Zeng}
\rhead{University of Southern California}
\cfoot{\thepage}

\usepackage{tikz}
\usetikzlibrary{positioning}
\begin{document}

My academic advisor once told me that while deep learning (DL) can be powerful, she still prefers to use more conventional methods for her research due to its lack of explainability. Her comment made me realize that although DL is a potent tool, it still has significant trade-offs that hinders its usage.
Specifically, DL models have a high capacity for generalizing patterns in data, but this comes at a trade off both in the lack of explainability of model predictions and reliance on the assumption that the data seen at inference time is i.i.d. with respect to the training data. This is especially problematic in domains such as medical imaging where explainability and fairness of model predictions is paramount; and furthermore some degree of domain-shift in data at inference time is expected. Hence, my current objective is to get a PhD to do research in industry with a focus on addressing this problem, i.e. creating robust, fair and explainable deep learning models.


I started self-studying machine learning at the end of my sophomore year of college when my interest in the intersection of linguistics and computer science led me to wonder how machine translation algorithms worked.
To gain practical experience, I pursued ML at a startup, SayKid, that used pre-built speech recognition models. There, I created a voice game for children premised on riddle solving using the Alexa Skills Kit. This experience underscored to me the necessity of designing robustly, as given the target audience of children, we needed to account for the unpredictability of how a child will interact with our game. At the same time, as the APIs I used abstracted away the ML model as a black box that takes in speech and returns text, it also gave me the desire to try research where I could work at a lower level with a greater focus on implementing and training models.

Hence, I started working with Professor David Liben-Nowell, who is studying human behavior through choice modeling. Here we quantified the effect of geographic location on people's choices -- specifically their ranking of US states by contribution to history. We used a Plackett-Luce model \cite{guiver2009bayesian} as a baseline and iterated upon it to test how a person's home state's geographic location affects their individual ranking.
While choice modeling is not ML, they both fundamentally involve data and model building.
%ties this part more explicitly in with my research interest
Furthermore, although choice models are limited in what they can describe, they are highly explainable due to the choice theories that underpin them. This is in contrast to data-driven DL models that are powerful but black-box due to their empirical nature.
I resonated with the intuitive nature of choice modeling and this juxtaposition propelled me to explore the problem of robustness and explainability in the context of DL. 


% re-edit this as it is too long and enumeration-y and U Chicago to the non U Chicago sop
To work more with robustness -- specifically in the medical imaging domain -- I participated in an REU program hosted by DePaul University. I was advised by Professor Daniela Raicu and focused on improving the domain-shift robustness of DL lung nodule classification models by identifying and training models over hidden stratification in types of lung nodules.
% i.e. with group distributionally robust optimization \cite{Sagawa*2020Distributionally}.
%wip
Specifically, I used both domain-driven and clustering methods to find semantically meaningful subgroupings of lung nodules. Then, I compared results from our baseline ResNet-18 models -- i.e. trained using transfer learning under the principle of Empirical Risk Minimization (ERM) -- to ResNet-18 models trained with group distributionally robust optimization (gDRO) \cite{Sagawa*2020Distributionally}. Through this, we concluded that we could increase model robustness in regard to the subgroups we discovered by using gDRO.
% my research abilities?
% The main difficulty in this experience was finding meaningful subgroupings of the data -- this specifically was as we were not radiologists. Thus solving these problems was very rewarding and one of the large driver for why I enjoy the research process.
%wip
In this experience, I was able to do research full time and was first author on a paper that was accepted \cite{zengNo2023}. This program fully convinced me that I wanted to do research
% and thus cemented my desire to pursue graduate school.
as I found the process of solving open-ended questions -- e.g. what exactly is a ``semantically meaningful'' stratification of our dataset -- a challenging and rewarding process.
It also again reinforced my interest in explainability and robustness as I saw first hand the need for more research in the subfields and how important it is in some ``high stakes'' domains e.g. the medical field where model predictions can greatly affect patient outcomes.
 
% While my journey into deep learning is still nascent, it has been fruitful and given me a clear direction.
To continue in this direction, I am now exploring this problem in other DL modalities -- specifically fairness in NLP. For my senior capstone project which is advised by Professor Anna Rafferty, I am looking at counterfactual fairness in language toxicity classification, re-implementing Counterfactual Logit Pairing \cite{garg2019counterfactual} and evaluating the robustness of these methods. Although language toxicity classification is an entirely different domain from my previous work of lung nodule malignancy classification, the same problem of robustness and fairness manifests here. Thus, in the future I would like to find more generalized methods to make deep learning models more transparent and accountable, 
%like how gdro and irm have been proposed?
as the opacity in deep learning models is a pervasive problem across different domains and modalities.
% I have also held ML workshops in the data science club at my school to help expose other people to ML in the hopes of bring other people to this field too.
%talk about 
% ROSA advice: mention more about what my future direction is?
To this end, I am applying for PhD programs as research is a fundamentally collaborative effort and not something I can develop on my own.
% Specifically, I wish to further develop my research ability in both finding meaningful questions and also producing useful results. 
This is with the goal of giving me the skills to eventually do research in industry e.g. with Google Jigsaw or OpenAI, where I can help ensure AI models being deployed in the world are used responsibly and equitably.
%centered on benefits and effects of ai on humans -- human centric research on ai

Therefore, I am applying to University of Southern California with an interest in working with either Professor Robin Jia, Professor Xiang Ren or Professor Yan Liu.
Professor Jia's research has a focus on NLP models robust to distribution shifts at test time. This closely aligns with my own longer term goal and desire to work with model robustness. Specifically, I am interested in his work on out of domain data streams \cite{lin-etal-2022-continual} and robustness to entity renaming \cite{yan-etal-2022-robustness}.
% explainability \cite{prasad-etal-2021-extent}.
Professor Ren similarly interests me with his work on model generalizablity e.g. explanation regularization \cite{joshi2022er} and also interpretability with faithful reasoners \cite{sanyal2022fairr}.
By working in either of the professors' labs, I can contribute with my knowledge of deep learning frameworks and strong coding experience. % the bs :(
Professor Liu interests me with her focus on medical fairness e.g. evaluating fairness in healthcare models \cite{meng2022interpretability} and also more generally with deepfake detection \cite{trinh2021examination}. My past work at my REU is similar to her current work and I would eager to continue this type of research with her.
Ultimately, by working with any of these professors, I aim to take advantage of the overlapping interests to further my own goals of working on machine learning models that are explainable, fair and robust.

% Robin Jia 
    % On Continual Model Refinement in Out-of-Distribution Data Streams
        % for OOD datastreams
    % On the Robustness of Reading Comprehension Models to Entity Renaming
        % robustness to entity renaming
    % To what extent do human explanations of model behavior align with actual model behavior?
        % importance alignment
% Xiang Ren Ink Research Lab
    % ER-TEST: Evaluating Explanation Regularization Methods for NLP Models
    % FaiRR: Faithful and Robust Deductive Reasoning over Natural Language
% yan liu Melady Lab
    % Interpretability and fairness evaluation of deep learning models on MIMIC-IV dataset
        % medical fairness
    % An Examination of Fairness of AI Models for Deepfake Detection
        % fairness
% \newpage
\bibliographystyle{acm}
\bibliography{refs} % Entries are in the refs.bib file



\end{document}
